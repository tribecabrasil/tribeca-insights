"""
CLI entrypoint for Tribeca Insights.

Orchestrates argument parsing, environment setup, crawling, and export.
"""

import argparse
import concurrent.futures
import hashlib
import json
import logging
import os
import re
import ssl
import time
import urllib.robotparser as robotparser
import xml.etree.ElementTree as ET
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import List, Set, Tuple
from urllib.error import URLError
from urllib.parse import urljoin, urlparse

import nltk
import pandas as pd
import requests
from bs4 import BeautifulSoup
from slugify import slugify

crawl_delay = 0.0

logger = logging.getLogger(__name__)

crawl_delay = 0.0


def get_crawl_delay(base_url: str) -> float:
    """Fetch crawl-delay from robots.txt or return 0."""
    parser = robotparser.RobotFileParser()
    parser.set_url(urljoin(base_url, "/robots.txt"))
    try:
        parser.read()
        delay = parser.crawl_delay("SeoCrawler")
        if delay is None:
            delay = parser.crawl_delay("*")
        return delay or 0.0
    except Exception:
        return 0.0


logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")


# Fun√ß√£o utilit√°ria para remover espa√ßos ou retornar string vazia, com logging para None
def safe_strip(value):
    if value is None:
        logger.debug("safe_strip recebeu None")
        return ""
    return value.strip() if isinstance(value, str) else ""


VERSION = "1.0"
USER_AGENT = f"SeoCrawler/{VERSION}"
CRAWLED_BY = USER_AGENT
session = requests.Session()
session.headers.update({"User-Agent": USER_AGENT})


# Configura o ambiente para permitir downloads do NLTK e evita erros SSL em algumas plataformas
def setup_environment() -> None:
    """Setup SSL context and download NLTK stopwords."""
    try:
        _create_unverified_https_context = ssl._create_unverified_context
        ssl._create_default_https_context = _create_unverified_https_context
    except AttributeError:
        pass
    nltk.download("stopwords", quiet=True)


# Limpa e tokeniza o texto, removendo caracteres n√£o alfab√©ticos, espa√ßos extras e stopwords.
# Esta fun√ß√£o unifica o tratamento de texto para an√°lise de frequ√™ncia, evitando redund√¢ncia.
def clean_and_tokenize(text: str, language: str = "english") -> List[str]:
    """Clean and tokenize text removing stopwords and short words."""
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    text = re.sub(r"\s+", " ", text)
    tokens = text.lower().split()
    stop_words = set(nltk.corpus.stopwords.words(language))
    return [word for word in tokens if word not in stop_words and len(word) > 2]


# Extrai o texto vis√≠vel de um HTML, removendo tags que n√£o contribuem para o conte√∫do principal
def extract_visible_text(html: str) -> str:
    """Extract visible text from HTML excluding non-content tags."""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "svg", "footer", "nav", "meta"]):
        tag.decompose()
    text = soup.get_text(separator=" ")
    return text.strip() if text else ""


# Obt√©m todos os links internos pertencentes ao dom√≠nio a partir do conte√∫do HTML
def get_internal_links(soup: BeautifulSoup, base_url: str, domain: str) -> Set[str]:
    """Get internal links from soup belonging to the domain."""
    links = set()
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if href.startswith("/") or domain in href:
            full_url = urljoin(base_url, href)
            if urlparse(full_url).netloc.replace("www.", "") == domain:
                links.add(full_url.split("#")[0])
    return links


# Obt√©m todos os links externos que n√£o pertencem ao dom√≠nio a partir do conte√∫do HTML
def get_external_links(soup: BeautifulSoup, domain: str) -> Set[str]:
    """Get external links from soup not belonging to the domain."""
    links = set()
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if href.startswith("http") and domain not in href:
            links.add(href)
    return links


# Carrega o arquivo CSV com URLs visitadas, ou cria um DataFrame vazio se n√£o existir
def load_visited_urls(csv_path: Path) -> pd.DataFrame:
    """Load visited URLs CSV or create empty DataFrame."""
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
    else:
        df = pd.DataFrame(columns=["URL", "Status", "Data"])
    if "MD File" not in df.columns:
        df["MD File"] = ""
    return df


# Salva o DataFrame de URLs visitadas em CSV, removendo duplicatas para manter integridade
def save_visited_urls(df: pd.DataFrame, csv_path: Path) -> None:
    """Save visited URLs DataFrame to CSV removing duplicates."""
    df = df.drop_duplicates(subset=["URL"])
    df.to_csv(csv_path, index=False)


# Solicita ao usu√°rio escolher um dom√≠nio j√° analisado ou digitar uma nova URL para iniciar o rastreamento
def ask_for_domain(existing_csvs: List[str]) -> Tuple[str, str]:
    """Ask user to select existing domain or input new URL. Also prompt for site language."""
    # Filter out any CSVs with empty domain (e.g., visited_urls_.csv)
    valid_csvs = []
    for f in existing_csvs:
        basename = os.path.basename(f)
        # match visited_urls_<non-empty>.csv
        m = re.match(r"visited_urls_(.+)\.csv$", basename)
        if m and m.group(1).strip():
            valid_csvs.append(f)
    domain_map = {}
    print("\nüìÅ Dom√≠nios j√° analisados:")
    for idx, file in enumerate(valid_csvs, 1):
        domain = re.match(r"visited_urls_(.+)\.csv$", os.path.basename(file)).group(1)
        domain_map[str(idx)] = domain
        print(f"{idx}. {domain}")
    print(f"{len(domain_map) + 1}. üîó Digitar nova URL")

    choice = input("\nEscolha uma op√ß√£o (n√∫mero): ").strip()
    if choice in domain_map:
        domain = domain_map[choice]
        base_url = f"https://{domain}"
    else:
        base_url = input(
            "\nüåê Digite a nova URL (ex: https://www.next-health.com): "
        ).strip()
        domain = urlparse(base_url).netloc.replace("www.", "")
    # Prompt for language
    print("\nüåê Qual o idioma principal do site? (en / pt-br)")
    site_language = input("site_language [en]: ").strip().lower()
    if not site_language:
        site_language = "en"
    return domain, base_url, site_language


# Cria a estrutura de pastas para armazenar os arquivos gerados durante a an√°lise
def setup_project_folder(domain: str) -> Path:
    """Create project folder and pages_md subfolder."""
    folder = Path(slugify(domain))
    folder.mkdir(exist_ok=True)
    (folder / "pages_md").mkdir(exist_ok=True)
    return folder


# Consulta o sitemap.xml do dom√≠nio para adicionar URLs novas ao DataFrame de URLs visitadas
def add_urls_from_sitemap(base_url: str, visited_df: pd.DataFrame) -> pd.DataFrame:
    """Add URLs from sitemap.xml to visited DataFrame."""
    sitemap_url = urljoin(base_url, "/sitemap.xml")
    try:
        resp = session.get(sitemap_url, timeout=10)
        if resp.status_code == 200:
            root = ET.fromstring(resp.content)
            ns = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}
            new_rows = []
            for url in root.findall(".//ns:loc", ns):
                if not url.text:
                    continue
                loc = url.text.strip()
                if loc not in visited_df["URL"].values:
                    new_rows.append({"URL": loc, "Status": 2, "Data": ""})
            if new_rows:
                new_df = pd.DataFrame(new_rows)
                return pd.concat([visited_df, new_df], ignore_index=True)
    except Exception as e:
        logger.warning(f"Erro ao acessar sitemap.xml: {e}")
    return visited_df


# Exporta o conte√∫do de uma p√°gina para um arquivo Markdown, incluindo t√≠tulo, descri√ß√£o, headings, texto, frequ√™ncia e imagens
def export_page_to_markdown(
    folder: Path, url: str, html: str, domain: str, external_links: Set[str]
) -> None:
    """Export page content to markdown file."""
    soup = BeautifulSoup(html, "html.parser")
    # Safe extraction of title
    try:
        title_tag = soup.title
        title = safe_strip(title_tag.string) if title_tag else "(sem t√≠tulo)"
    except Exception as e:
        logger.warning(f"[TITLE ERROR] {url}: {e}")
        title = "(erro ao extrair t√≠tulo)"

    # Safe extraction of meta description
    try:
        desc_tag = soup.find("meta", attrs={"name": "description"})
        desc_content = desc_tag.get("content") if desc_tag else None
        description = safe_strip(desc_content)
    except Exception as e:
        logger.warning(f"[META DESCRIPTION ERROR] {url}: {e}")
        description = "(erro ao extrair descri√ß√£o)"

    headings = [
        f"{'#' * int(tag.name[1])} {tag.get_text(strip=True)}"
        for tag in soup.find_all(re.compile("^h[1-6]$"))
    ]

    visible_text = extract_visible_text(html)
    tokens = clean_and_tokenize(visible_text)
    local_freq = Counter(tokens)

    images = soup.find_all("img")
    image_lines = []
    for img in images:
        src = img.get("src", "‚Äì")
        alt = safe_strip(img.get("alt")) or "_(sem ALT)_"
        image_lines.append(f"- `src`: {src}\n  - alt: {alt}")

    external = get_external_links(soup, domain)
    external_links.update(external)

    slug = slugify(urlparse(url).path or "home")
    filepath = folder / "pages_md" / f"{slug}.md"

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# An√°lise de P√°gina: `{url}`\n\n")
        f.write(f"**T√≠tulo:** {title}\n\n")
        f.write(f"**Meta Description:** {description}\n\n")
        f.write("## Hierarquia de Headings\n")
        f.write("\n".join(headings) if headings else "_Nenhum heading encontrado._")
        f.write("\n\n")
        f.write("## Conte√∫do Principal (limpo)\n")
        f.write(f"```\n{visible_text[:3000]}...\n```\n\n")
        f.write("## Frequ√™ncia de Palavras (top 20)\n")
        for word, freq in local_freq.most_common(20):
            f.write(f"- **{word}**: {freq}\n")
        f.write("\n## Imagens com ALT texts\n")
        f.write(
            "\n".join(image_lines) if image_lines else "_Nenhuma imagem encontrada._\n"
        )
        f.write("\n---\n")
        f.write(f"_Total de palavras analisadas: {len(tokens)}_\n")


def fetch_and_process(
    url: str, domain: str, folder: Path, language: str = "english"
) -> Tuple[str, Set[str], Tuple[str, str], str, dict]:
    """
    Fetch a URL, export markdown, and return:
        - visible text,
        - external links,
        - index entry tuple (slug, title),
        - markdown filename,
        - page_data (dict with all metrics for JSON export)
    """
    try:
        logger.info(f"Visitando: {url}")
        resp = session.get(url, timeout=10)
        time.sleep(crawl_delay)
        html = resp.text
        soup = BeautifulSoup(html, "html.parser")

        external_links: Set[str] = set()

        # Determina o nome do arquivo Markdown antes de exportar
        slug = slugify(urlparse(url).path or "home")
        md_filename = f"{slug}.md"

        export_page_to_markdown(folder, url, html, domain, external_links)
        visible_text = extract_visible_text(html)

        try:
            title_tag = soup.title
            title = safe_strip(title_tag.string) if title_tag else "(sem t√≠tulo)"
        except Exception as e:
            logger.warning(f"[TITLE ERROR - FETCH] {url}: {e}")
            title = "(erro ao extrair t√≠tulo)"
        index_entry = (slug, title)

        # Meta description
        try:
            desc_tag = soup.find("meta", attrs={"name": "description"})
            desc_content = desc_tag.get("content") if desc_tag else None
            description = safe_strip(desc_content)
        except Exception as e:
            logger.warning(f"[META DESCRIPTION ERROR - FETCH] {url}: {e}")
            description = "(erro ao extrair descri√ß√£o)"

        # Headings
        headings = [
            tag.get_text(strip=True) for tag in soup.find_all(re.compile("^h[1-6]$"))
        ]

        # Word frequency
        tokens = clean_and_tokenize(visible_text, language)
        local_freq = Counter(tokens)
        word_freq = dict(local_freq)

        # Images with alt
        images = soup.find_all("img")
        images_data = []
        for img in images:
            src = img.get("src", "‚Äì")
            alt = safe_strip(img.get("alt")) or ""
            images_data.append({"src": src, "alt": alt})

        # External links
        external = get_external_links(soup, domain)
        external_links.update(external)

        # Hash for page content
        page_hash = hashlib.sha256(visible_text.encode("utf-8")).hexdigest()

        page_data = {
            "url": url,
            "slug": slug,
            "title": title,
            "meta_description": description,
            "headings": headings,
            "word_count": len(tokens),
            "word_frequency": word_freq,
            "images": images_data,
            "external_links": sorted(list(external)),
            "page_hash": page_hash,
            "md_filename": md_filename,
        }

        return visible_text, external_links, index_entry, md_filename, page_data
    except Exception as e:
        logger.warning(f"Erro em {url}: {e}")
        return "", set(), None, None, None


# Realiza o rastreamento das p√°ginas a partir das URLs pendentes, atualiza status e coleta texto para an√°lise
def crawl_site(
    domain: str,
    base_url: str,
    folder: Path,
    visited_df: pd.DataFrame,
    max_pages: int,
    max_workers: int,
    language: str = "english",
) -> Tuple[str, list]:
    """
    Crawl site URLs concurrently, update visited_df, and return:
        - combined text corpus (for keyword frequency)
        - list of page_data dicts (for JSON export)
    """
    urls_to_visit = visited_df[visited_df["Status"] == 2]["URL"].tolist()[:max_pages]
    external_links: Set[str] = set()
    text_corpus: List[str] = []
    pages_data: List[dict] = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(fetch_and_process, url, domain, folder, language): url
            for url in urls_to_visit
        }
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                visible_text, ext_links, index_entry, md_filename, page_data = (
                    future.result()
                )
                if visible_text:
                    text_corpus.append(visible_text)
                external_links.update(ext_links)

                visited_df.loc[visited_df["URL"] == url, "Status"] = 1
                visited_df.loc[visited_df["URL"] == url, "Data"] = (
                    datetime.now().strftime("%Y-%m-%d")
                )
                if md_filename:
                    visited_df.loc[visited_df["URL"] == url, "MD File"] = md_filename
                if page_data:
                    pages_data.append(page_data)
            except Exception as e:
                logger.warning(f"Erro processando {url}: {e}")

    save_visited_urls(visited_df, folder / f"visited_urls_{domain}.csv")
    export_external_urls(folder, external_links)
    return " ".join(text_corpus), pages_data


# Atualiza e exporta a frequ√™ncia de palavras para um arquivo CSV, combinando dados anteriores e novos
def update_keyword_frequency(
    folder: Path, domain: str, full_text: str, language: str = "english"
) -> None:
    """Update and export keyword frequency CSV."""
    tokens = clean_and_tokenize(full_text, language)
    freq = Counter(tokens)

    csv_path = folder / f"keyword_frequency_{domain}.csv"
    if csv_path.exists():
        existing = pd.read_csv(csv_path)
        combined = Counter(dict(zip(existing["Word"], existing["Frequency"])))
        combined.update(freq)
        freq = combined

    df = pd.DataFrame(freq.items(), columns=["Word", "Frequency"]).sort_values(
        by="Frequency", ascending=False
    )
    df.to_csv(csv_path, index=False)
    logger.info(f"Frequ√™ncia exportada: {csv_path}")


# Exporta as URLs externas encontradas durante o rastreamento para um arquivo Markdown
def export_external_urls(folder: Path, external_links: Set[str]) -> None:
    """Export external URLs to markdown file."""
    with open(folder / "external_urls.md", "w", encoding="utf-8") as f:
        f.write("# URLs Externas Coletadas\n\n")
        for link in sorted(external_links):
            f.write(f"- {link}\n")


# Reconcilia MD File faltantes para garantir reprocessamento se necess√°rio
def reconcile_md_files(visited_df: pd.DataFrame, folder: Path) -> pd.DataFrame:
    """
    Para cada URL com Status 1 e MD File vazio, verifica se o arquivo .md existe em pages_md.
    - Se existir, preenche `MD File` com o nome do arquivo.
    - Se n√£o existir, redefine Status para 2 para reprocessamento.
    """
    for idx, row in visited_df.iterrows():
        if row["Status"] == 1 and not row["MD File"]:
            slug = slugify(urlparse(row["URL"]).path or "home")
            md_path = folder / "pages_md" / f"{slug}.md"
            if md_path.exists():
                visited_df.at[idx, "MD File"] = f"{slug}.md"
            else:
                visited_df.at[idx, "Status"] = 2
    return visited_df


# Gera um √≠ndice em Markdown com links para todas as p√°ginas analisadas, facilitando navega√ß√£o
def gerar_indice_markdown(folder: Path) -> None:
    """Generate markdown index file with links to analyzed pages."""
    index_path = folder / "index.md"
    pages = sorted((folder / "pages_md").glob("*.md"))
    with open(index_path, "w", encoding="utf-8") as f:
        f.write("# √çndice de P√°ginas Analisadas\n\n")
        for page in pages:
            title = page.stem.replace("-", " ").title()
            rel_path = page.relative_to(folder)
            f.write(f"- [{title}]({rel_path})\n")


# Fun√ß√£o principal que orquestra a execu√ß√£o completa do fluxo de crawling e an√°lise,
# incluindo setup, carregamento de hist√≥rico, rastreamento, an√°lise de palavras e gera√ß√£o de √≠ndices.
def main() -> None:
    """
    Main entrypoint: parse CLI args, set up environment, run crawl workflow,
    and export Markdown, CSV, and JSON artifacts.
    """
    setup_environment()

    parser = argparse.ArgumentParser()
    parser.add_argument("--max-pages", type=int, default=10)
    parser.add_argument("--language", type=str, default=None)
    parser.add_argument("--max-workers", type=int, default=5)
    parser.add_argument("--delay", type=float, default=None)
    args = parser.parse_args()

    existing_csvs = list(Path.cwd().glob("visited_urls_*.csv"))
    domain, base_url, site_language = ask_for_domain(existing_csvs)
    folder = setup_project_folder(domain)

    # Prefer CLI language if provided
    language = (
        args.language
        if args.language
        else ("portuguese" if site_language.startswith("pt") else "english")
    )

    global crawl_delay
    crawl_delay = args.delay if args.delay is not None else get_crawl_delay(base_url)

    visited_csv = folder / f"visited_urls_{domain}.csv"
    visited_df = load_visited_urls(visited_csv)
    # Reconcilia MD File faltantes para garantir reprocessamento se necess√°rio
    visited_df = reconcile_md_files(visited_df, folder)
    visited_df = add_urls_from_sitemap(base_url, visited_df)

    save_visited_urls(visited_df, visited_csv)

    project_created_at = datetime.now().isoformat()
    full_text, pages_data = crawl_site(
        domain,
        base_url,
        folder,
        visited_df,
        max_pages=args.max_pages,
        max_workers=args.max_workers,
        language=language,
    )
    update_keyword_frequency(folder, domain, full_text, language=language)
    gerar_indice_markdown(folder)

    # Export full JSON with all metadata and pages (with merging/updating)
    project_slug = slugify(domain)
    project_json_path = folder / f"project_{project_slug}.json"

    # Prepare project metadata
    project_created_at = project_created_at  # original timestamp
    project_updated_at = datetime.now().isoformat()

    # If a project JSON already exists, load and merge
    if project_json_path.exists():
        with open(project_json_path, "r", encoding="utf-8") as f:
            existing = json.load(f)
        # Preserve original creation date
        existing_created = existing.get("created_at", project_created_at)
        # Update metadata
        existing.update(
            {
                "version": VERSION,
                "crawled_by": CRAWLED_BY,
                "domain": domain,
                "base_url": base_url,
                "site_language": site_language,
                "language": language,
                "last_updated_at": project_updated_at,
                "max_pages": args.max_pages,
                "max_workers": args.max_workers,
                "crawl_delay": crawl_delay,
            }
        )
        # Merge pages: replace or add by slug
        pages_map = {p["slug"]: p for p in existing.get("pages", [])}
        for p in pages_data:
            pages_map[p["slug"]] = p
        existing["pages"] = list(pages_map.values())
        existing["pages_count"] = len(existing["pages"])
        project_data = existing
    else:
        # New project data
        project_data = {
            "version": VERSION,
            "crawled_by": CRAWLED_BY,
            "project_slug": project_slug,
            "domain": domain,
            "base_url": base_url,
            "site_language": site_language,
            "language": language,
            "created_at": project_created_at,
            "last_updated_at": project_created_at,
            "max_pages": args.max_pages,
            "max_workers": args.max_workers,
            "crawl_delay": crawl_delay,
            "pages_count": len(pages_data),
            "pages": pages_data,
        }

    # Write merged or new project JSON
    with open(project_json_path, "w", encoding="utf-8") as f:
        json.dump(project_data, f, ensure_ascii=False, indent=2)

    # Verifica se JSON foi criado com sucesso
    if project_json_path.exists():
        logger.info(f"JSON successfully written to: {project_json_path}")
    else:
        logger.error(f"Failed to write JSON: {project_json_path}")

    logger.info("Analysis completed.")
    return None


if __name__ == "__main__":
    main()
